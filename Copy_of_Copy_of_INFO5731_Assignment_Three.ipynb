{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of INFO5731_Assignment_Three.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshithamaddi/Akshitha_INFO5731_Spring2020/blob/master/Copy_of_Copy_of_INFO5731_Assignment_Three.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9",
        "colab_type": "text"
      },
      "source": [
        "# **INFO5731 Assignment Three**\n",
        "\n",
        "In this assignment, you are required to conduct information extraction, semantic analysis based on **the dataset you collected from assignment two**. You may use scipy and numpy package in this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF",
        "colab_type": "text"
      },
      "source": [
        "# **Question 1: Understand N-gram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k",
        "colab_type": "text"
      },
      "source": [
        "(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1oGrbspJupE",
        "colab_type": "code",
        "outputId": "60346385-3cd3-41f3-987b-780176179e00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('words')"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtKLmX3GWEFd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "a03a4d9f-253c-4ae8-8575-e57b9c094752"
      },
      "source": [
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "from selenium import webdriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.6/dist-packages (3.141.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,151 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,362 kB]\n",
            "Fetched 2,769 kB in 4s (727 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (80.0.3987.87-0ubuntu0.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 27 not upgraded.\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: use options instead of chrome_options\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RToeBlDbWOC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#scraping the data from web to obtain the file\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from selenium.webdriver.common.by import By\n",
        "options = webdriver.ChromeOptions()\n",
        "url = \"https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv\"\n",
        "driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "driver.get(url)\n",
        "time.sleep(10)\n",
        "Title = []\n",
        "date = []\n",
        "content= []\n",
        "page=0\n",
        "while True:\n",
        "    time.sleep(10)\n",
        "    html = driver.page_source\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    results = soup.find_all(\"div\", {\"class\": \"review-container\"})\n",
        "    if(page==3) :\n",
        "      for result in results:\n",
        "          Title.append(result.find(\"a\", {\"class\": \"title\"}).get_text().strip())\n",
        "          date.append(result.find(\"span\", {\"review-date\"}).get_text().strip())\n",
        "          content.append(result.find(\"div\", {\"class\": \"content\"}).get_text().strip())\n",
        "    if len(driver.find_elements_by_css_selector('.load-more-data')) > 0:\n",
        "          driver.find_element_by_css_selector('.load-more-data').click()\n",
        "          page+=1\n",
        "          if int(page)>3:\n",
        "           break\n",
        "    else:\n",
        "          break\n",
        "df = pd.DataFrame({\"Title\": Title, \"date\": date, \"content\": content})\n",
        "df.to_csv(\"output_review.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp59xOXZXBnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data Cleaning and Processing\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import Word\n",
        "\n",
        "ps = PorterStemmer()\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "pd.read_csv('output_review.csv', encoding='utf-8')\n",
        "df = pd.read_csv('output_review.csv')\n",
        "\n",
        "df.columns = ['','Title', 'date', 'content']\n",
        "#lowercase and removing special characters and punctuation marks\n",
        "df['Title'] = df['Title'].str.lower()\n",
        "df['content'] = df['content'].str.lower()\n",
        "df['Title']= df['Title'].str.replace(r'[^\\w\\s]+','')\n",
        "df['content']= df['content'].str.replace(r'[^\\w\\s]+','')\n",
        "#removing numbers\n",
        "df['Title'] = df['Title'].str.replace('\\d+', '')\n",
        "df['content'] = df['content'].str.replace('\\d+', '')\n",
        "#removing stopwords\n",
        "stop = stopwords.words('english')\n",
        "df['Title'] = df['Title'].apply(lambda x: [item for item in str.split(x) if item not in stop])\n",
        "df['content'] = df['content'].apply(lambda x: [item for item in str.split(x) if item not in stop])\n",
        "#lemmatization\n",
        "df['Title']= df['Title'].apply(lambda x : [lemma.lemmatize(y) for y in x])\n",
        "#df['Title'] = df['Title'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x]))\n",
        "df['content']= df['content'].apply(lambda x : [lemma.lemmatize(y) for y in x])\n",
        "#stemming\n",
        "df['Title']= df['Title'].apply(lambda x : [ps.stem(y) for y in x])\n",
        "df['content']= df['content'].apply(lambda x : [ps.stem(y) for y in x])\n",
        "\n",
        "def remove_punc(text):\n",
        "  no_punc=\" \".join([c for c in text if c not in string.punctuation])\n",
        "  return no_punc\n",
        "df['Title']= df['Title'].apply(lambda x : remove_punc(x))\n",
        "df['content']= df['content'].apply(lambda x : remove_punc(x))\n",
        "\n",
        "#Adding columns for clean_content and clean_title in a csv\n",
        "csv_input = pd.read_csv('output_review.csv')\n",
        "csv_input['clean_title'] = df['Title']\n",
        "csv_input['clean_content'] = df['content']\n",
        "csv_input.to_csv('output_review.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjsvJ59KJW1q",
        "colab_type": "code",
        "outputId": "8f6ae80b-b7e3-4be7-894a-d7669ac261ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "#frequency of 3-grams\n",
        "import pandas as pd \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "data = pd.read_csv('output_review.csv', low_memory=False)\n",
        "word_vectorizer = CountVectorizer(ngram_range=(3,3), analyzer='word')\n",
        "sparse_matrix = word_vectorizer.fit_transform(data['clean_title'].values.astype('U')) \n",
        "frequencies = sum(sparse_matrix).toarray()[0]\n",
        "df = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['Frequency'])\n",
        "df"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Frequency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>absolut masterpiec phoenix</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>account rate disappoint</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>account tell everyon</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>act perform ive</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>act perform phoenix</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worthi act perform</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>would call masterpiec</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>write review that</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>year best actor</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>yike peopl best</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>182 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                            Frequency\n",
              "absolut masterpiec phoenix          1\n",
              "account rate disappoint             1\n",
              "account tell everyon                1\n",
              "act perform ive                     1\n",
              "act perform phoenix                 1\n",
              "...                               ...\n",
              "worthi act perform                  1\n",
              "would call masterpiec               1\n",
              "write review that                   1\n",
              "year best actor                     1\n",
              "yike peopl best                     1\n",
              "\n",
              "[182 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN5ax_TGqTyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#collecting the required column from a csv file and saving it in text file\n",
        "#file is used to calculate the probabilities of bi-gram\n",
        "import csv\n",
        "with open('output_review.csv') as f:\n",
        " reader = csv.reader(f)\n",
        " next(reader, None)\n",
        " clean_title = [row[4] for row in reader]\n",
        "with open('clean_title.txt', mode=\"w\") as outfile:\n",
        "    for s in clean_title:\n",
        "        outfile.write(\"%s\\n\" % s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s_IdS5mz3Mq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculating probabilities of bi-grams using a function\n",
        "def bigramEstimation(file):\n",
        "    lst = []\n",
        "    unigrams = {}\n",
        "    bigrams = {} \n",
        "    text = open(file, 'r').read()\n",
        "    lst = text.strip().split()\n",
        "    del text \n",
        "    for l in lst:\n",
        "        if not l in unigrams:\n",
        "            unigrams[l] = 1\n",
        "        else:\n",
        "            unigrams[l] += 1\n",
        "    for i in range(len(lst) - 1):\n",
        "        temp = (lst[i], lst[i+1])\n",
        "        if not temp in bigrams:\n",
        "            bigrams[temp] = 1\n",
        "        else:\n",
        "          bigrams[temp] += 1\n",
        "    print('Generated ', len(bigrams), ' bigrams')\n",
        "    total_corpus = sum(unigrams.values())\n",
        "    for k,v in bigrams.items():\n",
        "        first_word = k[0]\n",
        "        first_word_count = unigrams[first_word]\n",
        "        bi_prob = bigrams[k] / unigrams[first_word]\n",
        "        if(v == 2):\n",
        "         print(k[0],k[1],v,bi_prob)        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yn3NZ7d3-10",
        "colab_type": "code",
        "outputId": "4c97b0d4-9690-4051-feeb-9e3464faa62f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "bigramEstimation('clean_title.txt')"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated  341  bigrams\n",
            "amaz movi 2 1.0\n",
            "joker best 2 0.14285714285714285\n",
            "one best 2 1.0\n",
            "act perform 2 0.6666666666666666\n",
            "ive ever 2 1.0\n",
            "extrem overr 2 1.0\n",
            "best comic 2 0.16666666666666666\n",
            "comic book 2 1.0\n",
            "perform phoenix 2 0.5\n",
            "best movi 2 0.16666666666666666\n",
            "dont get 2 0.2857142857142857\n",
            "real joker 2 0.4\n",
            "best dc 2 0.16666666666666666\n",
            "sinc dark 2 1.0\n",
            "movi year 2 0.11764705882352941\n",
            "masterpiec joker 2 0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVUHi1HD6zNR",
        "colab_type": "code",
        "outputId": "2caab884-f87c-48b4-dbcc-36d3ea6e7ce0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (46.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.38.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.21.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.38.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxWpHYFF6T7N",
        "colab_type": "code",
        "outputId": "209a7426-ffcc-4498-d86b-6350fb26739c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "#extracting nouns and calculating the noun frequencies \n",
        "import spacy\n",
        "import csv\n",
        "from tabulate import tabulate\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en\")\n",
        "file = open(\"clean_title.txt\", \"r\")\n",
        "doc = nlp(file.read())\n",
        "noun_phrases = []\n",
        "for np in doc.noun_chunks:\n",
        "  noun_phrases.append(np.text)\n",
        "print(noun_phrases)\n",
        "dfn = pd.DataFrame(noun_phrases, columns = ['noun_phrases'])\n",
        "word_vectorizer = CountVectorizer(ngram_range=(2,7), analyzer='word')\n",
        "sparse_matrix = word_vectorizer.fit_transform(dfn['noun_phrases'].values.astype('U')) \n",
        "frequencies = sum(sparse_matrix).toarray()[0]\n",
        "dff = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['Frequency'])\n",
        "dff['Noun_Probabilities'] = dff['Frequency'] / dff['Frequency'].max()\n",
        "result = dff.astype(object).transpose() \n",
        "print(tabulate(result, headers='keys', tablefmt='grid'))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['viewer actual', 'tiff', 'wit film', 'believ hype absolut masterpiec', 'phoenix', 'legend', 'outstand movi haunt', 'best charact', 'certain peopl relat\\nperfect everi aspect', 'masterpiec', 'amaz movi', 'psycholog studi', 'rather superhero flick', 'joaquin oscar joker', 'best dark suspens thriller darker dark knight', 'venic review', 'final real movi', 'good lord', 'spoon feed cgi fuel faux drama', 'real life', 'joker\\noscar', 'phoenix', 'critic', 'joker endgam', 'masterpiec', 'one best act', 'i', 'smile', 'overr', 'believ hype', 'masterpiec', 'that life', 'extrem overr', 'broken man', 'joker', 'brilliant best joker', 'i', 'movi', 'old peopl', 'clown princ crime arriv', 'probabl', 'one best comic book movi', 'phoenix', 'worthi act', 'phoenix', 'watch', 'joker', 'enough becom joker', 'we', 'account', 'everyon good', 'miser unpleas slog movi noth', 'stun\\nanyon rate movi poorli clearli', 'cinema', 'yike', 'bewar anyon hail masterpiec', 'astonish masterpiec', 'nonsens plot', 'everyon brain wash', 'account rate disappoint film', 'dc movi', 'hour full bad bad mood', 'who', 'joker', 'dc movi sinc dark knight', 'joketh movi', 'kind letdown', 'garbag hype', 'best movi year\\noverratedoverhyp', 'joke', 'oscar', 'review', 'overr', 'badli direct film', 'mislead titl', 'overhyp', 'proper entertain millenni joker', 'everyon els', 'insult best comic book', 'villain time', 'joker', 'masterpiec', 'best thing', 'dc', 'sinc dark knight', 'amaz movi', 'dark seriou', 'spectacular awesom fantast', 'cheap pretenti movi', 'dumb peopl', 'gener', 'good film', 'noth joker', 'great act', 'terribl film', 'overhyp', 'clear shelf oscar', 'hat joaquin phoenix', 'real joker', 'realli', 'realli disappoint', 'cinemat masterpiec', 'joker', 'outsid crimin', 'overact galor', 'uncomfort satisfi', 'dc movi date joaquin phoenix', 'joaquin phoenix deliv stori', 'overhyp movi', 'overr overhyp', 'masterpiec', 'joker']\n",
            "+--------------------+----------------------+----------------+---------------------------+--------------------------------+-------------+--------------+-------------------------+--------------+-------------------+--------------------------+----------------------------------+-----------------------+------------------+-----------+----------------+------------+----------------+---------------------+---------------+---------------+-----------------------+----------------------------------+------------+----------------+--------------+-------------------+------------------------+-------------+---------------------+------------------------------+-------------------------------------+------------------------------------------+-------------------------------------------------+--------------+-------------+------------------+-----------------------------------+--------------+---------------+--------------------+-------------------------------+-------------+--------------+------------------+------------------------+--------------+-----------------+-----------------------+-------------------------------+-------------------------------------+--------------------------------------------+------------+-----------------+-----------------------+------------------+-----------------------+----------------------+---------------+---------------------+---------------+---------------------+---------------------------+--------------+-------------------+---------------+---------------+---------------+----------------+-------------------------+--------------------------------+-------------------------------------+--------------------------------------------+---------------+----------------------+----------------+------------------------+-----------+----------------+------------------------+--------------------------------+----------------+---------------------+----------------------------+---------------+---------------+-------------------+--------------+----------------+----------------------+----------------------+----------------------------+----------------+-----------------+----------------------+---------------+----------------+----------------+--------------+------------+-----------------+----------------------+----------------------------+--------------+-------------------+-------------+-------------------+------------+----------------+---------------------+---------------+-------------+-------------+-------------+-------------------+---------------+-----------------------+-------------+-----------------+---------------------+--------------------------+----------------+---------------------------+---------------+---------------------+--------------------------+-----------------+-----------------------+-------------------+-------------------------+-------------------------------+----------------+---------------+---------------+----------------+------------------+-----------------+----------------------+---------------------------+--------------------------------+----------------+-------------+---------------------+-----------------------------+--------------+-------------+---------------+-----------------------+-------------+------------------+-------------------------+-------------+------------------------------+----------------+--------------+-------------+------------+----------------+------------------+-----------------------+----------------------------+---------------+-----------------+-----------------+-----------------------+-----------------+----------------+-----------------+---------------+-----------------------+-----------------------------+------------------------------------+-----------------+------------------------+-----------------+-----------------------+------------------+-----------------+---------------+---------------------+--------------------+-----------------------------+-----------------------------------+-------------------+-------------------+------------------------+-------------+--------------------+----------------------------+--------------------+--------------------------+--------------+-------------+-------------+---------------------+-----------------+-----------------------+------------------------------+---------------+-------------+--------------------+-------------+------------------+----------------------+------------------------------+--------------+------------------+-----------------------+----------------------------+----------------------------------+--------------+-------------------+------------------------+-------------------------------+---------------------------------------+-------------------+--------------------+---------------------------+--------------------------------+---------------------------------------+----------------+-------------+-------------------+------------------------+-------------------------------+---------------------+----------------+---------------------+--------------------------+----------------+-----------------+----------------+------------+--------------+-------------------------+\n",
            "|                    |   absolut masterpiec |   account rate |   account rate disappoint |   account rate disappoint film |   amaz movi |   anyon hail |   anyon hail masterpiec |   anyon rate |   anyon rate movi |   anyon rate movi poorli |   anyon rate movi poorli clearli |   astonish masterpiec |   awesom fantast |   bad bad |   bad bad mood |   bad mood |   badli direct |   badli direct film |   becom joker |   believ hype |   believ hype absolut |   believ hype absolut masterpiec |   best act |   best charact |   best comic |   best comic book |   best comic book movi |   best dark |   best dark suspens |   best dark suspens thriller |   best dark suspens thriller darker |   best dark suspens thriller darker dark |   best dark suspens thriller darker dark knight |   best joker |   best movi |   best movi year |   best movi year overratedoverhyp |   best thing |   bewar anyon |   bewar anyon hail |   bewar anyon hail masterpiec |   book movi |   brain wash |   brilliant best |   brilliant best joker |   broken man |   certain peopl |   certain peopl relat |   certain peopl relat perfect |   certain peopl relat perfect everi |   certain peopl relat perfect everi aspect |   cgi fuel |   cgi fuel faux |   cgi fuel faux drama |   cheap pretenti |   cheap pretenti movi |   cinemat masterpiec |   clear shelf |   clear shelf oscar |   clown princ |   clown princ crime |   clown princ crime arriv |   comic book |   comic book movi |   crime arriv |   dark knight |   dark seriou |   dark suspens |   dark suspens thriller |   dark suspens thriller darker |   dark suspens thriller darker dark |   dark suspens thriller darker dark knight |   darker dark |   darker dark knight |   date joaquin |   date joaquin phoenix |   dc movi |   dc movi date |   dc movi date joaquin |   dc movi date joaquin phoenix |   dc movi sinc |   dc movi sinc dark |   dc movi sinc dark knight |   deliv stori |   direct film |   disappoint film |   dumb peopl |   enough becom |   enough becom joker |   entertain millenni |   entertain millenni joker |   everi aspect |   everyon brain |   everyon brain wash |   everyon els |   everyon good |   extrem overr |   faux drama |   feed cgi |   feed cgi fuel |   feed cgi fuel faux |   feed cgi fuel faux drama |   final real |   final real movi |   fuel faux |   fuel faux drama |   full bad |   full bad bad |   full bad bad mood |   garbag hype |   good film |   good lord |   great act |   hail masterpiec |   hat joaquin |   hat joaquin phoenix |   hour full |   hour full bad |   hour full bad bad |   hour full bad bad mood |   hype absolut |   hype absolut masterpiec |   insult best |   insult best comic |   insult best comic book |   joaquin oscar |   joaquin oscar joker |   joaquin phoenix |   joaquin phoenix deliv |   joaquin phoenix deliv stori |   joker endgam |   joker oscar |   joketh movi |   kind letdown |   millenni joker |   miser unpleas |   miser unpleas slog |   miser unpleas slog movi |   miser unpleas slog movi noth |   mislead titl |   movi date |   movi date joaquin |   movi date joaquin phoenix |   movi haunt |   movi noth |   movi poorli |   movi poorli clearli |   movi sinc |   movi sinc dark |   movi sinc dark knight |   movi year |   movi year overratedoverhyp |   nonsens plot |   noth joker |   old peopl |   one best |   one best act |   one best comic |   one best comic book |   one best comic book movi |   oscar joker |   outsid crimin |   outstand movi |   outstand movi haunt |   overact galor |   overhyp movi |   overr overhyp |   peopl relat |   peopl relat perfect |   peopl relat perfect everi |   peopl relat perfect everi aspect |   perfect everi |   perfect everi aspect |   phoenix deliv |   phoenix deliv stori |   poorli clearli |   pretenti movi |   princ crime |   princ crime arriv |   proper entertain |   proper entertain millenni |   proper entertain millenni joker |   psycholog studi |   rate disappoint |   rate disappoint film |   rate movi |   rate movi poorli |   rate movi poorli clearli |   rather superhero |   rather superhero flick |   real joker |   real life |   real movi |   realli disappoint |   relat perfect |   relat perfect everi |   relat perfect everi aspect |   shelf oscar |   sinc dark |   sinc dark knight |   slog movi |   slog movi noth |   spectacular awesom |   spectacular awesom fantast |   spoon feed |   spoon feed cgi |   spoon feed cgi fuel |   spoon feed cgi fuel faux |   spoon feed cgi fuel faux drama |   stun anyon |   stun anyon rate |   stun anyon rate movi |   stun anyon rate movi poorli |   stun anyon rate movi poorli clearli |   superhero flick |   suspens thriller |   suspens thriller darker |   suspens thriller darker dark |   suspens thriller darker dark knight |   terribl film |   that life |   thriller darker |   thriller darker dark |   thriller darker dark knight |   uncomfort satisfi |   unpleas slog |   unpleas slog movi |   unpleas slog movi noth |   venic review |   viewer actual |   villain time |   wit film |   worthi act |   year overratedoverhyp |\n",
            "+====================+======================+================+===========================+================================+=============+==============+=========================+==============+===================+==========================+==================================+=======================+==================+===========+================+============+================+=====================+===============+===============+=======================+==================================+============+================+==============+===================+========================+=============+=====================+==============================+=====================================+==========================================+=================================================+==============+=============+==================+===================================+==============+===============+====================+===============================+=============+==============+==================+========================+==============+=================+=======================+===============================+=====================================+============================================+============+=================+=======================+==================+=======================+======================+===============+=====================+===============+=====================+===========================+==============+===================+===============+===============+===============+================+=========================+================================+=====================================+============================================+===============+======================+================+========================+===========+================+========================+================================+================+=====================+============================+===============+===============+===================+==============+================+======================+======================+============================+================+=================+======================+===============+================+================+==============+============+=================+======================+============================+==============+===================+=============+===================+============+================+=====================+===============+=============+=============+=============+===================+===============+=======================+=============+=================+=====================+==========================+================+===========================+===============+=====================+==========================+=================+=======================+===================+=========================+===============================+================+===============+===============+================+==================+=================+======================+===========================+================================+================+=============+=====================+=============================+==============+=============+===============+=======================+=============+==================+=========================+=============+==============================+================+==============+=============+============+================+==================+=======================+============================+===============+=================+=================+=======================+=================+================+=================+===============+=======================+=============================+====================================+=================+========================+=================+=======================+==================+=================+===============+=====================+====================+=============================+===================================+===================+===================+========================+=============+====================+============================+====================+==========================+==============+=============+=============+=====================+=================+=======================+==============================+===============+=============+====================+=============+==================+======================+==============================+==============+==================+=======================+============================+==================================+==============+===================+========================+===============================+=======================================+===================+====================+===========================+================================+=======================================+================+=============+===================+========================+===============================+=====================+================+=====================+==========================+================+=================+================+============+==============+=========================+\n",
            "| Frequency          |             1        |       1        |                  1        |                       1        |    2        |     1        |                1        |     1        |          1        |                 1        |                         1        |              1        |         1        |  1        |       1        |   1        |       1        |            1        |      1        |      2        |              1        |                         1        |   1        |       1        |     2        |          2        |               1        |    1        |            1        |                     1        |                            1        |                                 1        |                                        1        |     1        |    1        |         1        |                          1        |     1        |      1        |           1        |                      1        |    1        |     1        |         1        |               1        |     1        |        1        |              1        |                      1        |                            1        |                                   1        |   1        |        1        |              1        |         1        |              1        |             1        |      1        |            1        |      1        |            1        |                  1        |     2        |          1        |      1        |             3 |      1        |       1        |                1        |                       1        |                            1        |                                   1        |      1        |             1        |       1        |               1        |         3 |       1        |               1        |                       1        |       1        |            1        |                   1        |      1        |      1        |          1        |     1        |       1        |             1        |             1        |                   1        |       1        |        1        |             1        |      1        |       1        |       1        |     1        |   1        |        1        |             1        |                   1        |     1        |          1        |    1        |          1        |   1        |       1        |            1        |      1        |    1        |    1        |    1        |          1        |      1        |              1        |    1        |        1        |            1        |                 1        |       1        |                  1        |      1        |            1        |                 1        |        1        |              1        |                 3 |                1        |                      1        |       1        |      1        |      1        |       1        |         1        |        1        |             1        |                  1        |                       1        |       1        |    1        |            1        |                    1        |     1        |    1        |      1        |              1        |    1        |         1        |                1        |    1        |                     1        |       1        |     1        |    1        |   2        |       1        |         1        |              1        |                   1        |      1        |        1        |        1        |              1        |        1        |       1        |        1        |      1        |              1        |                    1        |                           1        |        1        |               1        |        1        |              1        |         1        |        1        |      1        |            1        |           1        |                    1        |                          1        |          1        |          1        |               1        |    1        |           1        |                   1        |           1        |                 1        |     1        |    1        |    1        |            1        |        1        |              1        |                     1        |      1        |    2        |           2        |    1        |         1        |             1        |                     1        |     1        |         1        |              1        |                   1        |                         1        |     1        |          1        |               1        |                      1        |                              1        |          1        |           1        |                  1        |                       1        |                              1        |       1        |    1        |          1        |               1        |                      1        |            1        |       1        |            1        |                 1        |       1        |        1        |       1        |   1        |     1        |                1        |\n",
            "+--------------------+----------------------+----------------+---------------------------+--------------------------------+-------------+--------------+-------------------------+--------------+-------------------+--------------------------+----------------------------------+-----------------------+------------------+-----------+----------------+------------+----------------+---------------------+---------------+---------------+-----------------------+----------------------------------+------------+----------------+--------------+-------------------+------------------------+-------------+---------------------+------------------------------+-------------------------------------+------------------------------------------+-------------------------------------------------+--------------+-------------+------------------+-----------------------------------+--------------+---------------+--------------------+-------------------------------+-------------+--------------+------------------+------------------------+--------------+-----------------+-----------------------+-------------------------------+-------------------------------------+--------------------------------------------+------------+-----------------+-----------------------+------------------+-----------------------+----------------------+---------------+---------------------+---------------+---------------------+---------------------------+--------------+-------------------+---------------+---------------+---------------+----------------+-------------------------+--------------------------------+-------------------------------------+--------------------------------------------+---------------+----------------------+----------------+------------------------+-----------+----------------+------------------------+--------------------------------+----------------+---------------------+----------------------------+---------------+---------------+-------------------+--------------+----------------+----------------------+----------------------+----------------------------+----------------+-----------------+----------------------+---------------+----------------+----------------+--------------+------------+-----------------+----------------------+----------------------------+--------------+-------------------+-------------+-------------------+------------+----------------+---------------------+---------------+-------------+-------------+-------------+-------------------+---------------+-----------------------+-------------+-----------------+---------------------+--------------------------+----------------+---------------------------+---------------+---------------------+--------------------------+-----------------+-----------------------+-------------------+-------------------------+-------------------------------+----------------+---------------+---------------+----------------+------------------+-----------------+----------------------+---------------------------+--------------------------------+----------------+-------------+---------------------+-----------------------------+--------------+-------------+---------------+-----------------------+-------------+------------------+-------------------------+-------------+------------------------------+----------------+--------------+-------------+------------+----------------+------------------+-----------------------+----------------------------+---------------+-----------------+-----------------+-----------------------+-----------------+----------------+-----------------+---------------+-----------------------+-----------------------------+------------------------------------+-----------------+------------------------+-----------------+-----------------------+------------------+-----------------+---------------+---------------------+--------------------+-----------------------------+-----------------------------------+-------------------+-------------------+------------------------+-------------+--------------------+----------------------------+--------------------+--------------------------+--------------+-------------+-------------+---------------------+-----------------+-----------------------+------------------------------+---------------+-------------+--------------------+-------------+------------------+----------------------+------------------------------+--------------+------------------+-----------------------+----------------------------+----------------------------------+--------------+-------------------+------------------------+-------------------------------+---------------------------------------+-------------------+--------------------+---------------------------+--------------------------------+---------------------------------------+----------------+-------------+-------------------+------------------------+-------------------------------+---------------------+----------------+---------------------+--------------------------+----------------+-----------------+----------------+------------+--------------+-------------------------+\n",
            "| Noun_Probabilities |             0.333333 |       0.333333 |                  0.333333 |                       0.333333 |    0.666667 |     0.333333 |                0.333333 |     0.333333 |          0.333333 |                 0.333333 |                         0.333333 |              0.333333 |         0.333333 |  0.333333 |       0.333333 |   0.333333 |       0.333333 |            0.333333 |      0.333333 |      0.666667 |              0.333333 |                         0.333333 |   0.333333 |       0.333333 |     0.666667 |          0.666667 |               0.333333 |    0.333333 |            0.333333 |                     0.333333 |                            0.333333 |                                 0.333333 |                                        0.333333 |     0.333333 |    0.333333 |         0.333333 |                          0.333333 |     0.333333 |      0.333333 |           0.333333 |                      0.333333 |    0.333333 |     0.333333 |         0.333333 |               0.333333 |     0.333333 |        0.333333 |              0.333333 |                      0.333333 |                            0.333333 |                                   0.333333 |   0.333333 |        0.333333 |              0.333333 |         0.333333 |              0.333333 |             0.333333 |      0.333333 |            0.333333 |      0.333333 |            0.333333 |                  0.333333 |     0.666667 |          0.333333 |      0.333333 |             1 |      0.333333 |       0.333333 |                0.333333 |                       0.333333 |                            0.333333 |                                   0.333333 |      0.333333 |             0.333333 |       0.333333 |               0.333333 |         1 |       0.333333 |               0.333333 |                       0.333333 |       0.333333 |            0.333333 |                   0.333333 |      0.333333 |      0.333333 |          0.333333 |     0.333333 |       0.333333 |             0.333333 |             0.333333 |                   0.333333 |       0.333333 |        0.333333 |             0.333333 |      0.333333 |       0.333333 |       0.333333 |     0.333333 |   0.333333 |        0.333333 |             0.333333 |                   0.333333 |     0.333333 |          0.333333 |    0.333333 |          0.333333 |   0.333333 |       0.333333 |            0.333333 |      0.333333 |    0.333333 |    0.333333 |    0.333333 |          0.333333 |      0.333333 |              0.333333 |    0.333333 |        0.333333 |            0.333333 |                 0.333333 |       0.333333 |                  0.333333 |      0.333333 |            0.333333 |                 0.333333 |        0.333333 |              0.333333 |                 1 |                0.333333 |                      0.333333 |       0.333333 |      0.333333 |      0.333333 |       0.333333 |         0.333333 |        0.333333 |             0.333333 |                  0.333333 |                       0.333333 |       0.333333 |    0.333333 |            0.333333 |                    0.333333 |     0.333333 |    0.333333 |      0.333333 |              0.333333 |    0.333333 |         0.333333 |                0.333333 |    0.333333 |                     0.333333 |       0.333333 |     0.333333 |    0.333333 |   0.666667 |       0.333333 |         0.333333 |              0.333333 |                   0.333333 |      0.333333 |        0.333333 |        0.333333 |              0.333333 |        0.333333 |       0.333333 |        0.333333 |      0.333333 |              0.333333 |                    0.333333 |                           0.333333 |        0.333333 |               0.333333 |        0.333333 |              0.333333 |         0.333333 |        0.333333 |      0.333333 |            0.333333 |           0.333333 |                    0.333333 |                          0.333333 |          0.333333 |          0.333333 |               0.333333 |    0.333333 |           0.333333 |                   0.333333 |           0.333333 |                 0.333333 |     0.333333 |    0.333333 |    0.333333 |            0.333333 |        0.333333 |              0.333333 |                     0.333333 |      0.333333 |    0.666667 |           0.666667 |    0.333333 |         0.333333 |             0.333333 |                     0.333333 |     0.333333 |         0.333333 |              0.333333 |                   0.333333 |                         0.333333 |     0.333333 |          0.333333 |               0.333333 |                      0.333333 |                              0.333333 |          0.333333 |           0.333333 |                  0.333333 |                       0.333333 |                              0.333333 |       0.333333 |    0.333333 |          0.333333 |               0.333333 |                      0.333333 |            0.333333 |       0.333333 |            0.333333 |                 0.333333 |       0.333333 |        0.333333 |       0.333333 |   0.333333 |     0.333333 |                0.333333 |\n",
            "+--------------------+----------------------+----------------+---------------------------+--------------------------------+-------------+--------------+-------------------------+--------------+-------------------+--------------------------+----------------------------------+-----------------------+------------------+-----------+----------------+------------+----------------+---------------------+---------------+---------------+-----------------------+----------------------------------+------------+----------------+--------------+-------------------+------------------------+-------------+---------------------+------------------------------+-------------------------------------+------------------------------------------+-------------------------------------------------+--------------+-------------+------------------+-----------------------------------+--------------+---------------+--------------------+-------------------------------+-------------+--------------+------------------+------------------------+--------------+-----------------+-----------------------+-------------------------------+-------------------------------------+--------------------------------------------+------------+-----------------+-----------------------+------------------+-----------------------+----------------------+---------------+---------------------+---------------+---------------------+---------------------------+--------------+-------------------+---------------+---------------+---------------+----------------+-------------------------+--------------------------------+-------------------------------------+--------------------------------------------+---------------+----------------------+----------------+------------------------+-----------+----------------+------------------------+--------------------------------+----------------+---------------------+----------------------------+---------------+---------------+-------------------+--------------+----------------+----------------------+----------------------+----------------------------+----------------+-----------------+----------------------+---------------+----------------+----------------+--------------+------------+-----------------+----------------------+----------------------------+--------------+-------------------+-------------+-------------------+------------+----------------+---------------------+---------------+-------------+-------------+-------------+-------------------+---------------+-----------------------+-------------+-----------------+---------------------+--------------------------+----------------+---------------------------+---------------+---------------------+--------------------------+-----------------+-----------------------+-------------------+-------------------------+-------------------------------+----------------+---------------+---------------+----------------+------------------+-----------------+----------------------+---------------------------+--------------------------------+----------------+-------------+---------------------+-----------------------------+--------------+-------------+---------------+-----------------------+-------------+------------------+-------------------------+-------------+------------------------------+----------------+--------------+-------------+------------+----------------+------------------+-----------------------+----------------------------+---------------+-----------------+-----------------+-----------------------+-----------------+----------------+-----------------+---------------+-----------------------+-----------------------------+------------------------------------+-----------------+------------------------+-----------------+-----------------------+------------------+-----------------+---------------+---------------------+--------------------+-----------------------------+-----------------------------------+-------------------+-------------------+------------------------+-------------+--------------------+----------------------------+--------------------+--------------------------+--------------+-------------+-------------+---------------------+-----------------+-----------------------+------------------------------+---------------+-------------+--------------------+-------------+------------------+----------------------+------------------------------+--------------+------------------+-----------------------+----------------------------+----------------------------------+--------------+-------------------+------------------------+-------------------------------+---------------------------------------+-------------------+--------------------+---------------------------+--------------------------------+---------------------------------------+----------------+-------------+-------------------+------------------------+-------------------------------+---------------------+----------------+---------------------+--------------------------+----------------+-----------------+----------------+------------+--------------+-------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z",
        "colab_type": "text"
      },
      "source": [
        "# **Question 2: Undersand TF-IDF and Document representation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw",
        "colab_type": "text"
      },
      "source": [
        "(40 points). Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program: \n",
        "\n",
        "(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab_type": "code",
        "outputId": "26786f1f-1c18-4584-c535-8f5b774bd29d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "#calculating the term-frequency\n",
        "df = pd.read_csv(\"output_review.csv\")\n",
        "tf2 = df.dropna()\n",
        "tf1 = (tf2['clean_title'].apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index())\n",
        "tf1.columns = ['words','tf']\n",
        "tf1"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>tf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>actual</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>legend</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>film</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>didnt</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wit</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>far</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>date</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>deliv</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>stori</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>depress</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>208 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       words   tf\n",
              "0     actual  1.0\n",
              "1     legend  1.0\n",
              "2       film  6.0\n",
              "3      didnt  2.0\n",
              "4        wit  1.0\n",
              "..       ...  ...\n",
              "203      far  1.0\n",
              "204     date  1.0\n",
              "205    deliv  1.0\n",
              "206    stori  1.0\n",
              "207  depress  1.0\n",
              "\n",
              "[208 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8j3n-KUsGaD",
        "colab_type": "code",
        "outputId": "db0c8c67-24a2-43f4-cac1-7b5cbd44bc7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "#building the document term weights(tf*idf)\n",
        "import numpy as np\n",
        "for i,word in enumerate(tf1['words']):\n",
        "  tf1.loc[i, 'idf'] = np.log(df.shape[0]/(len(tf2[tf2['clean_title'].str.contains(word)])))\n",
        "tf1['tf*idf'] = tf1['tf'] * tf1['idf']\n",
        "tf1"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>tf</th>\n",
              "      <th>idf</th>\n",
              "      <th>tf*idf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>actual</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.605170</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>legend</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.605170</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>film</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.813411</td>\n",
              "      <td>16.880464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>didnt</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.912023</td>\n",
              "      <td>7.824046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wit</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.605170</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>far</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.605170</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>date</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.605170</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>deliv</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.605170</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>stori</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.605170</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>depress</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.605170</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>208 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       words   tf       idf     tf*idf\n",
              "0     actual  1.0  4.605170   4.605170\n",
              "1     legend  1.0  4.605170   4.605170\n",
              "2       film  6.0  2.813411  16.880464\n",
              "3      didnt  2.0  3.912023   7.824046\n",
              "4        wit  1.0  4.605170   4.605170\n",
              "..       ...  ...       ...        ...\n",
              "203      far  1.0  4.605170   4.605170\n",
              "204     date  1.0  4.605170   4.605170\n",
              "205    deliv  1.0  4.605170   4.605170\n",
              "206    stori  1.0  4.605170   4.605170\n",
              "207  depress  1.0  4.605170   4.605170\n",
              "\n",
              "[208 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHo2G5Mw6rcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cosine similarity for all the documents\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import numpy.linalg as LA\n",
        "doc_set = tf2['clean_title'].values.tolist()\n",
        "query_set = \"As a viewer that actually went to TIFF and witnessed this film and didn't want to believe the hype, it is an absolute MASTERPIECE and Phoenix is a certified legend.\"\n",
        "query_set = [query_set]\n",
        "stopWords = stopwords.words('english')\n",
        "vectorizer = CountVectorizer(stop_words = stopWords)\n",
        "transformer = TfidfTransformer()\n",
        "docVectorizerArray = vectorizer.fit_transform(doc_set).toarray()\n",
        "queryVectorizerArray = vectorizer.transform(query_set).toarray()\n",
        "cx = lambda a, b : np.inner(a, b)/(LA.norm(a)*LA.norm(b))\n",
        "result = []\n",
        "for vector in docVectorizerArray:\n",
        "        for testV in queryVectorizerArray:\n",
        "            cosine = cx(vector, testV)\n",
        "            result.append(cosine)           "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I69wiJ7ikBos",
        "colab_type": "code",
        "outputId": "8d4ffba7-1e52-4817-d5d3-7142f93722e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "source": [
        "#ranking the dicuments based on  cosine similarity\n",
        "new = tf2.filter(['Unnamed','clean_title'], axis=1)\n",
        "se = pd.Series(result)\n",
        "new['Cosine_similarity'] = se.values\n",
        "new.drop(new.loc[new['Cosine_similarity']==0].index, inplace=True)\n",
        "new[\"Rank\"] = new[\"Cosine_similarity\"].rank().astype(int)\n",
        "new.sort_values(\"Cosine_similarity\", inplace = True) \n",
        "new"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_title</th>\n",
              "      <th>Cosine_similarity</th>\n",
              "      <th>Rank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>worthi act perform phoenix worth watch joker s...</td>\n",
              "      <td>0.088388</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>probabl one best comic book movi perform phoen...</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>far best dc movi date joaquin phoenix perfect</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>masterpiec movi year best actor joaquin phoenix</td>\n",
              "      <td>0.133631</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>went blind didnt enjoy realli disappoint</td>\n",
              "      <td>0.144338</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>overr badli direct film mislead titl</td>\n",
              "      <td>0.144338</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>joaquin phoenix deliv stori doesnt</td>\n",
              "      <td>0.158114</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>made account rate disappoint film</td>\n",
              "      <td>0.158114</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>went second time watch</td>\n",
              "      <td>0.176777</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>great act terribl film</td>\n",
              "      <td>0.176777</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>good film noth joker</td>\n",
              "      <td>0.176777</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>hat joaquin phoenix</td>\n",
              "      <td>0.204124</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>dont believ hype</td>\n",
              "      <td>0.204124</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>garbag hype</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>ok film</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>oscar phoenix</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hype real</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>believ hype</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>viewer actual went tiff wit film didnt want be...</td>\n",
              "      <td>0.730297</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          clean_title  Cosine_similarity  Rank\n",
              "37  worthi act perform phoenix worth watch joker s...           0.088388     1\n",
              "36  probabl one best comic book movi perform phoen...           0.102062     2\n",
              "93      far best dc movi date joaquin phoenix perfect           0.125000     3\n",
              "83    masterpiec movi year best actor joaquin phoenix           0.133631     4\n",
              "88           went blind didnt enjoy realli disappoint           0.144338     5\n",
              "66               overr badli direct film mislead titl           0.144338     5\n",
              "94                 joaquin phoenix deliv stori doesnt           0.158114     7\n",
              "48                  made account rate disappoint film           0.158114     7\n",
              "7                              went second time watch           0.176777    10\n",
              "80                             great act terribl film           0.176777    10\n",
              "79                               good film noth joker           0.176777    10\n",
              "84                                hat joaquin phoenix           0.204124    12\n",
              "60                                   dont believ hype           0.204124    12\n",
              "56                                        garbag hype           0.250000    16\n",
              "22                                            ok film           0.250000    16\n",
              "15                                      oscar phoenix           0.250000    16\n",
              "4                                           hype real           0.250000    16\n",
              "25                                        believ hype           0.250000    16\n",
              "0   viewer actual went tiff wit film didnt want be...           0.730297    19"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV",
        "colab_type": "text"
      },
      "source": [
        "# **Question 3: Create your own training and evaluation data for sentiment analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX",
        "colab_type": "text"
      },
      "source": [
        "(15 points). **You dodn't need to write program for this question!** Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfvMKJjIXS5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "https://github.com/akshithamaddi/Akshitha_INFO5731_Spring2020/blob/master/Assignment5_File_5731.csv"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}