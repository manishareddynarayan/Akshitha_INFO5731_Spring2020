{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy_of_INFO5731_Assignment_Three (7).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshithamaddi/Akshitha_INFO5731_Spring2020/blob/master/Copy_of_INFO5731_Assignment_Three_(7).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9",
        "colab_type": "text"
      },
      "source": [
        "# **INFO5731 Assignment Four**\n",
        "\n",
        "In this assignment, you are required to conduct topic modeling, sentiment analysis based on **the dataset you created from assignment three**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF",
        "colab_type": "text"
      },
      "source": [
        "# **Question 1: Topic Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k",
        "colab_type": "text"
      },
      "source": [
        "(30 points). This question is designed to help you develop a feel for the way topic modeling works, the connection to the human meanings of documents. Based on the dataset from assignment three, write a python program to **identify the top 10 topics in the dataset**. Before answering this question, please review the materials in lesson 8, especially the code for LDA and LSA. The following information should be reported:\n",
        "\n",
        "(1) Features (top n-gram phrases) used for topic modeling.\n",
        "\n",
        "(2) Top 10 clusters for topic modeling.\n",
        "\n",
        "(3) Summarize and describe the topic for each cluster. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KjhWSd-bL1L",
        "colab_type": "code",
        "outputId": "d395d864-9be0-422d-9a68-f02125b57c52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "import pandas as pd\n",
        "papers = pd.read_csv(\"Assignment5_File_5731.csv\",encoding = \"ISO-8859-1\")\n",
        "papers_data = papers['Clean_Title']\n",
        "papers.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Document_ID</th>\n",
              "      <th>Clean_Title</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>viewer actual went tiff wit film didnt want be...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>outstand movi haunt perform best charact devel...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>certain peopl relat</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>perfect everi aspect</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>hype real</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Document_ID                                        Clean_Title Sentiment\n",
              "0            1  viewer actual went tiff wit film didnt want be...  Positive\n",
              "1            2  outstand movi haunt perform best charact devel...  Positive\n",
              "2            3                                certain peopl relat   Neutral\n",
              "3            4                               perfect everi aspect  Positive\n",
              "4            5                                          hype real  Positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve1fSsezcrDo",
        "colab_type": "code",
        "outputId": "95e01186-9dca-4672-90b9-257317d6d8e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "data = papers.Clean_Title.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['viewer', 'actual', 'went', 'tiff', 'wit', 'film', 'didnt', 'want', 'believ', 'hype', 'absolut', 'masterpiec', 'phoenix', 'certifi', 'legend'], ['outstand', 'movi', 'haunt', 'perform', 'best', 'charact', 'develop', 'ever', 'seen'], ['certain', 'peopl', 'relat'], ['perfect', 'everi', 'aspect'], ['hype', 'real'], ['masterpiec'], ['amaz', 'movi', 'exist'], ['went', 'second', 'time', 'watch'], ['psycholog', 'studi', 'rather', 'superhero', 'flick'], ['joaquin', 'oscar', 'joker', 'best', 'dark', 'suspens', 'thriller', 'darker', 'dark', 'knight'], ['venic', 'review'], ['represint', 'real', 'life', 'joker'], ['final', 'real', 'movi'], ['good', 'lord'], ['spoon', 'feed', 'cgi', 'fuel', 'faux', 'drama'], ['oscar', 'phoenix'], ['critic', 'useless'], ['joker', 'endgam'], ['would', 'call', 'masterpiec'], ['one', 'best', 'act', 'perform', 'ive', 'ever', 'seen'], ['dont', 'forget', 'smile'], ['ok', 'film'], ['extrem', 'overr'], ['believ', 'hype'], ['masterpiec'], ['great', 'dont', 'know'], ['that', 'life'], ['extrem', 'overr'], ['requiem', 'broken', 'man'], ['rise', 'joker'], ['brilliant', 'best', 'joker', 'ive', 'ever', 'seen'], ['movi', 'decad'], ['omg', 'get', 'old', 'peopl', 'get', 'stupid'], ['clown', 'princ', 'crime', 'arriv'], ['probabl', 'one', 'best', 'comic', 'book', 'movi', 'perform', 'phoenix', 'make', 'speechless', 'end', 'venic'], ['worthi', 'act', 'perform', 'phoenix', 'worth', 'watch', 'joker', 'smart', 'enough', 'becom', 'joker', 'weve', 'come', 'know'], ['saw', 'tonight', 'make', 'account', 'tell', 'everyon', 'good'], ['miser', 'unpleas', 'slog', 'movi', 'noth'], ['stun'], ['anyon', 'rate', 'movi', 'poorli', 'clearli', 'doesnt', 'appreci', 'cinema'], ['bewar', 'anyon', 'hail', 'masterpiec'], ['yike', 'peopl', 'best', 'movi', 'ever'], ['dont', 'get'], ['astonish', 'masterpiec'], ['feel', 'like', 'everyon', 'brain', 'wash'], ['nonsens', 'plot'], ['made', 'account', 'rate', 'disappoint', 'film'], ['stop', 'compar', 'endgam'], ['dc', 'movi'], ['hour', 'full', 'bad', 'bad', 'mood'], ['who', 'real', 'joker'], ['best', 'dc', 'movi', 'sinc', 'dark', 'knight', 'rise'], ['joke', 'movi'], ['kind', 'letdown'], ['best', 'movi', 'year'], ['dont', 'believ', 'hype'], ['garbag', 'hype'], ['oscar', 'sooner', 'later'], [], ['joke'], ['perfect'], ['first', 'time', 'write', 'review', 'that', 'good'], ['mesmer'], ['dont', 'sheep'], ['overr', 'badli', 'direct', 'film', 'mislead', 'titl'], ['overhyp', 'overact', 'proper', 'entertain', 'millenni', 'joker'], ['dont', 'get', 'everyon', 'els', 'seem', 'see'], ['insult', 'best', 'comic', 'book', 'villain', 'time'], ['joker', 'laughter', 'joke', 'matter'], ['masterpiec'], ['best', 'thing', 'dc', 'done', 'sinc', 'dark', 'knight'], ['amaz', 'movi'], ['overr', 'bore', 'cheap', 'pretenti', 'movi', 'make', 'dumb', 'peopl', 'feel', 'smarter'], ['dark', 'seriou', 'tens', 'bad'], ['spectacular', 'awesom', 'fantast'], ['gener'], ['good', 'film', 'noth', 'joker'], ['great', 'act', 'terribl', 'film'], ['overhyp'], ['clear', 'shelf', 'oscar'], ['masterpiec', 'movi', 'year', 'best', 'actor', 'joaquin', 'phoenix'], ['hat', 'joaquin', 'phoenix'], ['real', 'joker'], ['fell', 'asleep'], ['realli', 'dont', 'understand'], ['went', 'blind', 'didnt', 'enjoy', 'realli', 'disappoint'], ['cinemat', 'masterpiec'], ['joker', 'outsid', 'crimin'], ['overact', 'galor'], ['uncomfort', 'satisfi'], ['far', 'best', 'dc', 'movi', 'date', 'joaquin', 'phoenix', 'perfect'], ['joaquin', 'phoenix', 'deliv', 'stori', 'doesn'], ['dark', 'depress'], ['overhyp', 'movi'], ['overr', 'overhyp'], ['masterpiec'], ['joker'], ['deriv', 'uninspir']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxv47ZkkwVgO",
        "colab_type": "code",
        "outputId": "0a34f7a8-2c02-4eb8-dd06-28d728403dcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CThlj8TTxKnG",
        "colab_type": "code",
        "outputId": "18e7ac67-63eb-44e1-f68d-66466979b459",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et6pCAnkzSBV",
        "colab_type": "code",
        "outputId": "d8633fb5-62d3-4019-97f6-b9e00ac813d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import spacy\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "print(data_lemmatized)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['go', 'want', 'believ', 'hype'], ['perform', 'good', 'charact', 'develop', 'ever', 'see'], ['certain', 'peopl', 'relat'], ['perfect', 'everi', 'aspect'], ['hype'], [], ['exist'], ['go', 'second', 'time', 'watch'], ['rather', 'superhero'], ['good', 'dark', 'suspen'], ['review'], ['real', 'life', 'joker'], ['final', 'real', 'movi'], ['good'], ['feed', 'cgi', 'fuel', 'faux', 'drama'], [], ['critic', 'useless'], [], ['would', 'call'], ['good', 'act', 'perform', 'have', 'ever', 'see'], ['forget', 'smile'], ['film'], ['extrem', 'overr'], [], [], ['great', 'know'], ['life'], ['extrem', 'overr'], ['requiem', 'broken', 'man'], ['rise'], ['brilliant', 'best', 'joker', 'have', 'ever', 'see'], [], ['old', 'peopl', 'get', 'stupid'], [], ['probabl', 'good', 'comic', 'book', 'perform', 'make', 'speechless', 'end', 'venic'], ['perform', 'have', 'come', 'know'], ['see', 'tonight', 'make', 'account', 'tell', 'everyon', 'good'], [], [], ['anyon', 'rate', 'cinema'], [], ['best', 'movi', 'ever'], [], ['astonish'], ['feel', 'everyon', 'brain', 'wash'], [], ['make', 'account', 'rate', 'disappoint', 'film'], ['stop', 'compar', 'endgam'], [], ['hour', 'full', 'bad', 'bad', 'mood'], ['real'], ['good'], ['joke'], [], ['good', 'year'], ['believ', 'hype'], [], ['sooner', 'later'], [], ['joke'], ['perfect'], ['first', 'time', 'write', 'review', 'good'], [], ['sheep'], ['film'], [], ['everyon', 'seem'], ['good', 'comic', 'book', 'time'], [], [], ['good', 'thing'], [], ['cheap', 'pretenti', 'movi', 'make', 'dumb', 'peopl', 'feel', 'smart'], ['ten', 'bad'], [], [], ['good'], [], [], [], ['good', 'actor'], [], ['real'], ['fall', 'asleep'], ['realli', 'understand'], ['go', 'blind', 'enjoy', 'realli', 'disappoint'], ['cinemat'], [], [], [], ['far', 'good'], [], [], [], [], [], [], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNg3sxKyDXYn",
        "colab_type": "code",
        "outputId": "0b2fb325-ad4f-4042-ed3e-d3ef0a813c81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import gensim.corpora as corpora\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "texts = data_lemmatized\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "print(corpus)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[(0, 1), (1, 1), (2, 1), (3, 1)], [(4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)], [(10, 1), (11, 1), (12, 1)], [(13, 1), (14, 1), (15, 1)], [(2, 1)], [], [(16, 1)], [(1, 1), (17, 1), (18, 1), (19, 1)], [(20, 1), (21, 1)], [(7, 1), (22, 1), (23, 1)], [(24, 1)], [(25, 1), (26, 1), (27, 1)], [(27, 1), (28, 1), (29, 1)], [(7, 1)], [(30, 1), (31, 1), (32, 1), (33, 1), (34, 1)], [], [(35, 1), (36, 1)], [], [(37, 1), (38, 1)], [(6, 1), (7, 1), (8, 1), (9, 1), (39, 1), (40, 1)], [(41, 1), (42, 1)], [(43, 1)], [(44, 1), (45, 1)], [], [], [(46, 1), (47, 1)], [(26, 1)], [(44, 1), (45, 1)], [(48, 1), (49, 1), (50, 1)], [(51, 1)], [(6, 1), (9, 1), (25, 1), (40, 1), (52, 1), (53, 1)], [], [(11, 1), (54, 1), (55, 1), (56, 1)], [], [(7, 1), (8, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1)], [(8, 1), (40, 1), (47, 1), (64, 1)], [(7, 1), (9, 1), (60, 1), (65, 1), (66, 1), (67, 1), (68, 1)], [], [], [(69, 1), (70, 1), (71, 1)], [], [(6, 1), (29, 1), (52, 1)], [], [(72, 1)], [(66, 1), (73, 1), (74, 1), (75, 1)], [], [(43, 1), (60, 1), (65, 1), (71, 1), (76, 1)], [(77, 1), (78, 1), (79, 1)], [], [(80, 2), (81, 1), (82, 1), (83, 1)], [(27, 1)], [(7, 1)], [(84, 1)], [], [(7, 1), (85, 1)], [(0, 1), (2, 1)], [], [(86, 1), (87, 1)], [], [(84, 1)], [(15, 1)], [(7, 1), (18, 1), (24, 1), (88, 1), (89, 1)], [], [(90, 1)], [(43, 1)], [], [(66, 1), (91, 1)], [(7, 1), (18, 1), (57, 1), (58, 1)], [], [], [(7, 1), (92, 1)], [], [(11, 1), (29, 1), (60, 1), (74, 1), (93, 1), (94, 1), (95, 1), (96, 1)], [(80, 1), (97, 1)], [], [], [(7, 1)], [], [], [], [(7, 1), (98, 1)], [], [(27, 1)], [(99, 1), (100, 1)], [(101, 1), (102, 1)], [(1, 1), (76, 1), (101, 1), (103, 1), (104, 1)], [(105, 1)], [], [], [], [(7, 1), (106, 1)], [], [], [], [], [], [], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96Xg5ySFDfz0",
        "colab_type": "code",
        "outputId": "e1d394a3-c605-4613-aa3d-2dc8c102ecf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "from pprint import pprint\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=10, \n",
        "                                       random_state=100,\n",
        "                                       chunksize=100,\n",
        "                                       passes=10,\n",
        "                                       per_word_topics=True)\n",
        "pprint(lda_model.print_topics(num_words=4))\n",
        "doc_lda = lda_model[corpus]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, '0.112*\"real\" + 0.112*\"perfect\" + 0.112*\"joke\" + 0.059*\"aspect\"'),\n",
            " (1, '0.064*\"good\" + 0.064*\"time\" + 0.043*\"make\" + 0.043*\"movi\"'),\n",
            " (2, '0.097*\"everyon\" + 0.051*\"feel\" + 0.051*\"broken\" + 0.051*\"wash\"'),\n",
            " (3, '0.151*\"good\" + 0.062*\"bad\" + 0.062*\"film\" + 0.033*\"best\"'),\n",
            " (4, '0.080*\"superhero\" + 0.080*\"rather\" + 0.080*\"rise\" + 0.007*\"good\"'),\n",
            " (5, '0.089*\"overr\" + 0.089*\"extrem\" + 0.046*\"know\" + 0.046*\"perform\"'),\n",
            " (6, '0.032*\"account\" + 0.032*\"tonight\" + 0.032*\"tell\" + 0.032*\"faux\"'),\n",
            " (7, '0.093*\"good\" + 0.048*\"ever\" + 0.048*\"peopl\" + 0.048*\"perform\"'),\n",
            " (8, '0.089*\"hype\" + 0.061*\"good\" + 0.060*\"believ\" + 0.032*\"film\"'),\n",
            " (9, '0.085*\"realli\" + 0.085*\"life\" + 0.045*\"go\" + 0.045*\"disappoint\"')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNckib26EyWf",
        "colab_type": "text"
      },
      "source": [
        "**1.Features used for Topic Modelling**                                         \n",
        "1.Tokenizing the sentences into list of words and removing the stopwords, unnecesarry characters if any.                                                  \n",
        "2.Gensim's Phrases Model can implement the bigrams,trigrams,quadgrams and more.The two important argument for the phrases are min_count and threshold.If the threshold is higher we get fewer phrases.                                       \n",
        "3.The two main inputs of the LDA topic model are dictionary and the corpus.Gensim creates a unique id for each word in the document.The produced corpus shown above is a mapping of(word_id,word_frequency).                            \n",
        "4.**LDA MODEL:**After creating the dictionary and corpus,we need to provide the num_topics as well.                                                              \n",
        "I used chucksize and passes while building LDA model using gensims.Chunksize decides how many documents are processed at a time in the training algorithm and passes decides how often we can train the model on the entire corpus.                     \n",
        "5.The above LDA model is build with the top 10 topics where each topic is the combination of keywords and each keyword contributes a certain weightage to the topic.                                                                           \n",
        "lda_model.print_topics(num_words=4) is used to print the keywords of each topic and the importance(weightage) of each keyword with 4 topic terms for each topic.\n",
        "\n",
        "**2.Top 10 Clusters for topic Modeling**                                        \n",
        "The top 10 clusters obtained are:                                              \n",
        "(0, '0.112*\"real\" + 0.112*\"perfect\" + 0.112*\"joke\" + 0.059*\"aspect\"'),      \n",
        " (1, '0.064*\"good\" + 0.064*\"time\" + 0.043*\"make\" + 0.043*\"movi\"'),      \n",
        " (2, '0.097*\"everyon\" + 0.051*\"feel\" + 0.051*\"broken\" + 0.051*\"wash\"'),     \n",
        " (3, '0.151*\"good\" + 0.062*\"bad\" + 0.062*\"film\" + 0.033*\"best\"'),    \n",
        " (4, '0.080*\"superhero\" + 0.080*\"rather\" + 0.080*\"rise\" + 0.007*\"good\"'),    \n",
        " (5, '0.089*\"overr\" + 0.089*\"extrem\" + 0.046*\"know\" + 0.046*\"perform\"'),   \n",
        " (6, '0.032*\"account\" + 0.032*\"tonight\" + 0.032*\"tell\" + 0.032*\"faux\"'),   \n",
        " (7, '0.093*\"good\" + 0.048*\"ever\" + 0.048*\"peopl\" + 0.048*\"perform\"'),     \n",
        " (8, '0.089*\"hype\" + 0.061*\"good\" + 0.060*\"believ\" + 0.032*\"film\"'),  \n",
        " (9, '0.085*\"realli\" + 0.085*\"life\" + 0.045*\"go\" + 0.045*\"disappoint\"')]\n",
        "\n",
        " **3.Summarize and describe the topics for each cluster**  \n",
        " 0.Joker is a real hype with every perfect aspect.                             \n",
        " 1.Spend good time and best movie making.                                        \n",
        " 2.Everyone feel like broken and brain wash.                                     \n",
        " 3.Bad film and insult to best comic book.                                       \n",
        " 4.Psychological study rather than a superhero.                                     \n",
        " 5.Over extreme performance.                                                     \n",
        " 6.Saw tonight and made an account to tell everyone about faux drama.            \n",
        " 7.One act performance of people I have ever seen.                               \n",
        " 8.Believe the hype,Good film to watch.                                          \n",
        " 9.Went blind dint enjoy that life of joker really disappointed.\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z",
        "colab_type": "text"
      },
      "source": [
        "# **Question 2: Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw",
        "colab_type": "text"
      },
      "source": [
        "(30 points). Sentiment analysis also known as opinion mining is a sub field within Natural Language Processing (NLP) that builds machine learning algorithms to classify a text according to the sentimental polarities of opinions it contains, e.g., positive, negative, neutral. The purpose of this question is to develop a machine learning classifier for sentiment analysis. Based on the dataset from assignment three, write a python program to implement a sentiment classifier and evaluate its performance. Notice: **80% data for training and 20% data for testing**.  \n",
        "\n",
        "(1) Features used for sentiment classification and explain why you select these features.\n",
        "\n",
        "(2) Select two of the supervised learning algorithm from scikit-learn library: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning, to build a sentiment classifier respectively. \n",
        "\n",
        "(3) Compare the performance over accuracy, precision, recall, and F1 score for the two algorithms you selected. Here is the reference of how to calculate these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write your code here\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "df = pd.read_csv('Assignment5_File_5731.csv',encoding = \"ISO-8859-1\")\n",
        "X = df['Clean_Title']\n",
        "y = df['Sentiment']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LkeMz-FDqK_",
        "colab_type": "code",
        "outputId": "08b882e8-a296-470a-941d-ea662b46d85e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "#LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
        "\n",
        "vect = CountVectorizer(min_df=2, ngram_range=(1, 2))\n",
        "X_train = vect.fit(X_train).transform(X_train) \n",
        "X_test = vect.transform(X_test)\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train,y_train)\n",
        "y_pred=logreg.predict(X_test)\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_pred,average = 'macro'))\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_pred,average = 'macro'))\n",
        "print(\"f1 score:\",metrics.f1_score(y_test, y_pred,average = 'macro'))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7\n",
            "Precision: 0.47222222222222215\n",
            "Recall: 0.48888888888888893\n",
            "f1 score: 0.47771836007130125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRZltEclI5M5",
        "colab_type": "code",
        "outputId": "4e5fb69a-69c3-413c-cbd9-05960f9ad411",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "#support Vector Machine\n",
        "from sklearn import svm\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
        "vect = CountVectorizer(min_df=2, ngram_range=(1, 2))\n",
        "X_train = vect.fit(X_train).transform(X_train) \n",
        "X_test = vect.transform(X_test)\n",
        "clf = svm.SVC(kernel='linear')\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_pred,average = 'macro'))\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_pred,average = 'macro'))\n",
        "print(\"f1 score:\",metrics.f1_score(y_test, y_pred,average = 'macro'))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5\n",
            "Precision: 0.3790849673202614\n",
            "Recall: 0.36296296296296293\n",
            "f1 score: 0.30769230769230765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9itLtnnN5uOy",
        "colab_type": "text"
      },
      "source": [
        "**1.Features used for Sentiment Classification**\n",
        "A feature or aspect is an attribute of entity.Different features can generate different sentiment analysis.\n",
        "The features used for sentiment classification are clean titles of the joker movie reviews and sentiment i.e. if the title is positive,negative or neutral.I chose these features to obtain the better accuarcy because the accuracy depends on the relatinship between the features.\n",
        "\n",
        "For feature selection we need to divide the dataset into dependent and independent variables.Here dependent is the sentiment columns and independent is the clean title from our dataset.\n",
        "\n",
        "Some of the techniques used for feature selection are Mutual Information,Chi-Square,Information gain and TF-idf to select geatures from high dimentionality of feature set.\n",
        "\n",
        "**Classification of sentiment:**\n",
        "![alt text](https://www.kdnuggets.com/images/sentiment-fig-2-532.jpg)\n",
        "\n",
        "\n",
        "***1.LogisticRegression***\n",
        "Logistic Regression is one of the most simple and commonly used Machine Learning algorithms for two-class classification.It describes and estimates the relationship between one dependent binary variable and independent variables.    \n",
        "1.I have dividied the columns into two types variables dependent(y) and independent variable(X).\n",
        "2.splitting the dataset into 80% training data and 20% test data using function train_test_split().In this function we need to pass three parameters dependent variable,independent variable and test_size and random_state can be used to select the records randomly.\n",
        "3.LogisticRegression() function is used to create a Logistic Regression classifier object.\n",
        "4.Fit() function is used to fit the model on the train set and predict() function is used to make predictions on the test set.\n",
        "5.Accurcay,Precision,recall and f1 score are calculated to evaluate the metrics.                               \n",
        "***2.Support Vector Machine***                                                   \n",
        "SVM constructs a hyperplane in multidimensional space to separate different classes. SVM generates optimal hyperplane in an iterative manner, which is used to minimize an error. The core idea of SVM is to find a maximum marginal hyperplane(MMH) that best divides the dataset into classes.\n",
        "1.I have dividied the columns into two types variables dependent(y) and independent variable(X).\n",
        "2.splitting the dataset into 80% training data and 20% test data using function train_test_split().In this function we need to pass three parameters dependent variable,independent variable and test_size and random_state can be used to select the records randomly.\n",
        "3.SVC() function is used to create a Support Vector classifier object by passing an argument kernel as the linear kernel.A linear Kernel is nothing but a dot product of any two given observations.\n",
        "4.Fit() function is used to fit the model on the train set and predict() function is used to make predictions on the test set.\n",
        "5.Accurcay,Precision,recall and f1 score are calculated to evaluate the metrics.\n",
        "\n",
        "**3.Comparing the algorithms based on accuracy,precision,recall amd f1 score**\n",
        "\n",
        "I have chosen the logistic regression and support vector machine algorithms because our dataset is very small and for accurate results over a small dataset we have few algorithms like logistic regression, SVM using linear, Naive Bayes, RandomForest.\n",
        "\n",
        "From the baove results based on the accuracy,precision,recall and f1 score we can say that logistic regression classifier is best suited for this dataset as the accuracy is 70% which is good enough and the f1 score is close to 0.5 which means the algorithm is good for this dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV",
        "colab_type": "text"
      },
      "source": [
        "# **Question 3: House price prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX",
        "colab_type": "text"
      },
      "source": [
        "(40 points). You are required to build a **regression** model to predict the house price with 79 explanatory variables describing (almost) every aspect of residential homes. The purpose of this question is to practice regression analysis, an supervised learning model. The training data, testing data, and data description files can be download here: https://github.com/unt-iialab/INFO5731_Spring2020/blob/master/Assignments/assignment4-question3-data.zip. Here is an axample for implementation: https://towardsdatascience.com/linear-regression-in-python-predict-the-bay-areas-home-price-5c91c8378878. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfvMKJjIXS5G",
        "colab_type": "code",
        "outputId": "355f3f0a-215d-48e9-95cc-d445b9e33b65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "train_X = train.drop('SalePrice', axis=1)\n",
        "train_y = train.SalePrice\n",
        "test_X = test\n",
        "onehot_train_X = pd.get_dummies(train_X)\n",
        "onehot_test_X = pd.get_dummies(test_X)\n",
        "train_X, test_X = onehot_train_X.align(onehot_test_X, join='left', axis=1)\n",
        "my_imputer = SimpleImputer()\n",
        "train_X = my_imputer.fit_transform(train_X)\n",
        "test_X = my_imputer.transform(test_X)\n",
        "reg = LinearRegression()\n",
        "cv_scores = cross_val_score(reg, train_X, train_y, cv=5)\n",
        "reg = LinearRegression()\n",
        "reg.fit(train_X, train_y)\n",
        "predictions = reg.predict(test_X)\n",
        "my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice':predictions})\n",
        "my_submission.to_csv(\"Predicted_House_Price.csv\")\n",
        "my_submission"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1461</td>\n",
              "      <td>112709.903813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1462</td>\n",
              "      <td>161010.363863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1463</td>\n",
              "      <td>186961.179807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1464</td>\n",
              "      <td>197300.132022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1465</td>\n",
              "      <td>205541.292943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1454</th>\n",
              "      <td>2915</td>\n",
              "      <td>86095.528874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455</th>\n",
              "      <td>2916</td>\n",
              "      <td>81610.279372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1456</th>\n",
              "      <td>2917</td>\n",
              "      <td>181080.339264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1457</th>\n",
              "      <td>2918</td>\n",
              "      <td>117021.339522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1458</th>\n",
              "      <td>2919</td>\n",
              "      <td>225002.291367</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1459 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Id      SalePrice\n",
              "0     1461  112709.903813\n",
              "1     1462  161010.363863\n",
              "2     1463  186961.179807\n",
              "3     1464  197300.132022\n",
              "4     1465  205541.292943\n",
              "...    ...            ...\n",
              "1454  2915   86095.528874\n",
              "1455  2916   81610.279372\n",
              "1456  2917  181080.339264\n",
              "1457  2918  117021.339522\n",
              "1458  2919  225002.291367\n",
              "\n",
              "[1459 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsTNhfXN83cJ",
        "colab_type": "code",
        "outputId": "e177ba10-8c07-4ce4-d474-9d8453b2aed7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#Performance evaluation for the baove model.\n",
        "reg = LinearRegression()\n",
        "reg.fit(train_X,train_y)\n",
        "X_train , X_test, Y_train, Y_test = train_test_split(\n",
        "    train_X,train_y,\n",
        "        test_size=0.20,\n",
        "        random_state=42)\n",
        "predictions = reg.predict(X_test)\n",
        "print(\"Mean Absolute Error\", metrics.mean_absolute_error(Y_test, predictions))\n",
        "print(\"Mean Square Error\",metrics.mean_squared_error(Y_test, predictions))\n",
        "print(\"R2 Score\",metrics.r2_score(Y_test, predictions))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Absolute Error 14426.740961107349\n",
            "Mean Square Error 522409274.01144296\n",
            "R2 Score 0.931892169915545\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}