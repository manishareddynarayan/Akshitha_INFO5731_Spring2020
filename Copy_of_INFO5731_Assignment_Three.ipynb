{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of INFO5731_Assignment_Three.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshithamaddi/Akshitha_INFO5731_Spring2020/blob/master/Copy_of_INFO5731_Assignment_Three.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9",
        "colab_type": "text"
      },
      "source": [
        "# **INFO5731 Assignment Three**\n",
        "\n",
        "In this assignment, you are required to conduct information extraction, semantic analysis based on **the dataset you collected from assignment two**. You may use scipy and numpy package in this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF",
        "colab_type": "text"
      },
      "source": [
        "# **Question 1: Understand N-gram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k",
        "colab_type": "text"
      },
      "source": [
        "(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1oGrbspJupE",
        "colab_type": "code",
        "outputId": "8f899b91-f7c0-44f4-9f8a-e72f39cd96c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjsvJ59KJW1q",
        "colab_type": "code",
        "outputId": "5f245410-12b9-473e-8739-d8b450b6e817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "#frequency of 3-grams\n",
        "import pandas as pd \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "data = pd.read_csv('output_review.csv', low_memory=False)\n",
        "word_vectorizer = CountVectorizer(ngram_range=(3,3), analyzer='word')\n",
        "sparse_matrix = word_vectorizer.fit_transform(data['clean_title'].values.astype('U')) \n",
        "frequencies = sum(sparse_matrix).toarray()[0]\n",
        "df = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['Frequency'])\n",
        "df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Frequency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>absolut masterpiec phoenix</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>account rate disappoint</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>account tell everyon</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>act perform ive</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>act perform phoenix</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>worthi act perform</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>would call masterpiec</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>write review that</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>year best actor</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>yike peopl best</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>182 rows Ã— 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                            Frequency\n",
              "absolut masterpiec phoenix          1\n",
              "account rate disappoint             1\n",
              "account tell everyon                1\n",
              "act perform ive                     1\n",
              "act perform phoenix                 1\n",
              "...                               ...\n",
              "worthi act perform                  1\n",
              "would call masterpiec               1\n",
              "write review that                   1\n",
              "year best actor                     1\n",
              "yike peopl best                     1\n",
              "\n",
              "[182 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN5ax_TGqTyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#collecting the required column from a csv file and saving it in text file\n",
        "#file is used to calculate the probabilities of bi-gram\n",
        "import csv\n",
        "with open('output_review.csv') as f:\n",
        " reader = csv.reader(f)\n",
        " next(reader, None)\n",
        " clean_title = [row[4] for row in reader]\n",
        "with open('clean_title.txt', mode=\"w\") as outfile:\n",
        "    for s in clean_title:\n",
        "        outfile.write(\"%s\\n\" % s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s_IdS5mz3Mq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculating probabilities of bi-grams using a function\n",
        "def bigramEstimation(file):\n",
        "    lst = []\n",
        "    unigrams = {}\n",
        "    bigrams = {} \n",
        "    text = open(file, 'r').read()\n",
        "    lst = text.strip().split()\n",
        "    del text \n",
        "    for l in lst:\n",
        "        if not l in unigrams:\n",
        "            unigrams[l] = 1\n",
        "        else:\n",
        "            unigrams[l] += 1\n",
        "    for i in range(len(lst) - 1):\n",
        "        temp = (lst[i], lst[i+1])\n",
        "        if not temp in bigrams:\n",
        "            bigrams[temp] = 1\n",
        "        else:\n",
        "          bigrams[temp] += 1\n",
        "    print('Generated ', len(bigrams), ' bigrams')\n",
        "    total_corpus = sum(unigrams.values())\n",
        "    for k,v in bigrams.items():\n",
        "        first_word = k[0]\n",
        "        first_word_count = unigrams[first_word]\n",
        "        bi_prob = bigrams[k] / unigrams[first_word]\n",
        "        if(v == 2):\n",
        "         print(k[0],k[1],v,bi_prob)        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yn3NZ7d3-10",
        "colab_type": "code",
        "outputId": "4ae843e5-4d33-4f25-937b-200d4d464d59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "bigramEstimation('clean_title.txt')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated  341  bigrams\n",
            "amaz movi 2 1.0\n",
            "joker best 2 0.14285714285714285\n",
            "one best 2 1.0\n",
            "act perform 2 0.6666666666666666\n",
            "ive ever 2 1.0\n",
            "extrem overr 2 1.0\n",
            "best comic 2 0.16666666666666666\n",
            "comic book 2 1.0\n",
            "perform phoenix 2 0.5\n",
            "best movi 2 0.16666666666666666\n",
            "dont get 2 0.2857142857142857\n",
            "real joker 2 0.4\n",
            "best dc 2 0.16666666666666666\n",
            "sinc dark 2 1.0\n",
            "movi year 2 0.11764705882352941\n",
            "movi overr 2 0.11764705882352941\n",
            "masterpiec joker 2 0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVUHi1HD6zNR",
        "colab_type": "code",
        "outputId": "4dfe5024-d7ff-43dd-bdee-d5b20e65b48e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.38.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (46.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.38.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.21.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxWpHYFF6T7N",
        "colab_type": "code",
        "outputId": "3a2227ec-7f52-467c-897a-2ecbef705ba3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#extracting nouns and calculating the noun frequencies \n",
        "#I have chose a noun phrases with 3 words.\n",
        "import spacy\n",
        "import csv\n",
        "\n",
        "nlp = spacy.load(\"en\")\n",
        "file = open(\"clean_title.txt\", \"r\")\n",
        "doc = nlp(file.read())\n",
        "noun_phrases = []\n",
        "for np in doc.noun_chunks:\n",
        "  noun_phrases.append(np.text)\n",
        "print(noun_phrases)\n",
        "dfn = pd.DataFrame(noun_phrases, columns = ['noun_phrases'])\n",
        "word_vectorizer = CountVectorizer(ngram_range=(3,3), analyzer='word')\n",
        "sparse_matrix = word_vectorizer.fit_transform(dfn['noun_phrases'].values.astype('U')) \n",
        "frequencies = sum(sparse_matrix).toarray()[0]\n",
        "dff = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['Frequency'])\n",
        "dff['Noun_Probabilities'] = dff / dff.max()\n",
        "dff\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['viewer actual', 'tiff', 'wit film', 'believ hype absolut masterpiec', 'phoenix', 'legend', 'outstand movi haunt', 'best charact', 'certain peopl relat\\nperfect everi aspect', 'masterpiec', 'amaz movi', 'psycholog studi', 'rather superhero flick', 'joaquin oscar joker', 'best dark suspens thriller darker dark knight', 'venic review', 'represint', 'real life', 'joker', 'final real movi', 'good lord', 'spoon feed cgi fuel faux drama', 'oscar phoenix\\ncritic', 'joker endgam', 'masterpiec', 'one best act', 'i', 'smile', 'film', 'extrem overr', 'believ hype', 'masterpiec', 'that life', 'extrem overr', 'broken man', 'joker', 'brilliant best joker', 'i', 'movi', 'old peopl', 'clown princ crime arriv', 'probabl', 'one best comic book movi', 'phoenix', 'worthi act', 'phoenix', 'watch', 'joker', 'enough becom joker', 'we', 'account', 'everyon good', 'miser unpleas slog movi noth', 'stun\\nanyon rate movi poorli clearli', 'cinema', 'bewar anyon hail masterpiec', 'yike', 'astonish masterpiec', 'everyon brain wash', 'nonsens plot', 'account rate disappoint film', 'dc movi', 'hour full bad bad mood', 'who', 'joker', 'dc movi sinc dark knight', 'joketh movi', 'kind letdown', 'garbag hype', 'oscar', 'overratedoverhyp\\njoke', 'review', 'overr', 'badli direct film', 'mislead titl', 'overhyp', 'proper entertain millenni joker', 'everyon els', 'insult best comic book', 'villain time', 'joker', 'masterpiec', 'best thing', 'dc', 'sinc dark knight', 'amaz movi', 'cheap pretenti movi', 'dumb peopl', 'dark seriou', 'spectacular awesom fantast', 'gener', 'good film', 'noth joker', 'great act', 'terribl film', 'overhyp', 'clear shelf oscar', 'hat joaquin phoenix', 'real joker', 'realli', 'realli disappoint', 'cinemat masterpiec', 'joker', 'outsid crimin', 'overact galor', 'uncomfort satisfi', 'dc movi date joaquin phoenix', 'joaquin phoenix deliv stori', 'overhyp movi', 'overr overhyp', 'masterpiec', 'joker', 'deriv uninspir']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Frequency</th>\n",
              "      <th>Noun_Probabilities</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>account rate disappoint</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>anyon hail masterpiec</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>anyon rate movi</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bad bad mood</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>badli direct film</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>believ hype absolut</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>best comic book</th>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>best dark suspens</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bewar anyon hail</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>brilliant best joker</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>certain peopl relat</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cgi fuel faux</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cheap pretenti movi</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>clear shelf oscar</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>clown princ crime</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>comic book movi</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dark suspens thriller</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>darker dark knight</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date joaquin phoenix</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dc movi date</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dc movi sinc</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>enough becom joker</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>entertain millenni joker</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>everyon brain wash</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>feed cgi fuel</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>final real movi</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fuel faux drama</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>full bad bad</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hat joaquin phoenix</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hour full bad</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hype absolut masterpiec</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>insult best comic</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>joaquin oscar joker</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>joaquin phoenix deliv</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>miser unpleas slog</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>movi date joaquin</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>movi poorli clearli</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>movi sinc dark</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>one best act</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>one best comic</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oscar phoenix critic</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>outstand movi haunt</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>peopl relat perfect</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perfect everi aspect</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>phoenix deliv stori</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>princ crime arriv</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>proper entertain millenni</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rate disappoint film</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rate movi poorli</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rather superhero flick</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>relat perfect everi</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sinc dark knight</th>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>slog movi noth</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>spectacular awesom fantast</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>spoon feed cgi</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stun anyon rate</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>suspens thriller darker</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thriller darker dark</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unpleas slog movi</th>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            Frequency  Noun_Probabilities\n",
              "account rate disappoint             1                 0.5\n",
              "anyon hail masterpiec               1                 0.5\n",
              "anyon rate movi                     1                 0.5\n",
              "bad bad mood                        1                 0.5\n",
              "badli direct film                   1                 0.5\n",
              "believ hype absolut                 1                 0.5\n",
              "best comic book                     2                 1.0\n",
              "best dark suspens                   1                 0.5\n",
              "bewar anyon hail                    1                 0.5\n",
              "brilliant best joker                1                 0.5\n",
              "certain peopl relat                 1                 0.5\n",
              "cgi fuel faux                       1                 0.5\n",
              "cheap pretenti movi                 1                 0.5\n",
              "clear shelf oscar                   1                 0.5\n",
              "clown princ crime                   1                 0.5\n",
              "comic book movi                     1                 0.5\n",
              "dark suspens thriller               1                 0.5\n",
              "darker dark knight                  1                 0.5\n",
              "date joaquin phoenix                1                 0.5\n",
              "dc movi date                        1                 0.5\n",
              "dc movi sinc                        1                 0.5\n",
              "enough becom joker                  1                 0.5\n",
              "entertain millenni joker            1                 0.5\n",
              "everyon brain wash                  1                 0.5\n",
              "feed cgi fuel                       1                 0.5\n",
              "final real movi                     1                 0.5\n",
              "fuel faux drama                     1                 0.5\n",
              "full bad bad                        1                 0.5\n",
              "hat joaquin phoenix                 1                 0.5\n",
              "hour full bad                       1                 0.5\n",
              "hype absolut masterpiec             1                 0.5\n",
              "insult best comic                   1                 0.5\n",
              "joaquin oscar joker                 1                 0.5\n",
              "joaquin phoenix deliv               1                 0.5\n",
              "miser unpleas slog                  1                 0.5\n",
              "movi date joaquin                   1                 0.5\n",
              "movi poorli clearli                 1                 0.5\n",
              "movi sinc dark                      1                 0.5\n",
              "one best act                        1                 0.5\n",
              "one best comic                      1                 0.5\n",
              "oscar phoenix critic                1                 0.5\n",
              "outstand movi haunt                 1                 0.5\n",
              "peopl relat perfect                 1                 0.5\n",
              "perfect everi aspect                1                 0.5\n",
              "phoenix deliv stori                 1                 0.5\n",
              "princ crime arriv                   1                 0.5\n",
              "proper entertain millenni           1                 0.5\n",
              "rate disappoint film                1                 0.5\n",
              "rate movi poorli                    1                 0.5\n",
              "rather superhero flick              1                 0.5\n",
              "relat perfect everi                 1                 0.5\n",
              "sinc dark knight                    2                 1.0\n",
              "slog movi noth                      1                 0.5\n",
              "spectacular awesom fantast          1                 0.5\n",
              "spoon feed cgi                      1                 0.5\n",
              "stun anyon rate                     1                 0.5\n",
              "suspens thriller darker             1                 0.5\n",
              "thriller darker dark                1                 0.5\n",
              "unpleas slog movi                   1                 0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z",
        "colab_type": "text"
      },
      "source": [
        "# **Question 2: Undersand TF-IDF and Document representation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw",
        "colab_type": "text"
      },
      "source": [
        "(40 points). Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program: \n",
        "\n",
        "(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab_type": "code",
        "outputId": "28160fb4-3128-4a9a-e29e-bebd925fcf0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "#calculating the term-frequency\n",
        "df = pd.read_csv(\"output_review.csv\")\n",
        "tf2 = df.dropna()\n",
        "tf1 = (tf2['clean_title'].apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index())\n",
        "tf1.columns = ['words','tf']\n",
        "tf1"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>tf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>want</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tiff</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>didnt</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>went</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>viewer</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>deliv</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>stori</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>depress</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>uninspir</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209</th>\n",
              "      <td>deriv</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>210 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        words   tf\n",
              "0        want  1.0\n",
              "1        tiff  1.0\n",
              "2       didnt  2.0\n",
              "3        went  3.0\n",
              "4      viewer  1.0\n",
              "..        ...  ...\n",
              "205     deliv  1.0\n",
              "206     stori  1.0\n",
              "207   depress  1.0\n",
              "208  uninspir  1.0\n",
              "209     deriv  1.0\n",
              "\n",
              "[210 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8j3n-KUsGaD",
        "colab_type": "code",
        "outputId": "1d33d8bb-20f2-469c-d32c-07921d0babf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "#building the document term weights(tf*idf)\n",
        "import numpy as np\n",
        "for i,word in enumerate(tf1['words']):\n",
        "  tf1.loc[i, 'idf'] = np.log(df.shape[0]/(len(tf2[tf2['clean_title'].str.contains(word)])))\n",
        "tf1['tf*idf'] = tf1['tf'] * tf1['idf']\n",
        "tf1"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>tf</th>\n",
              "      <th>idf</th>\n",
              "      <th>tf*idf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>want</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.605170</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tiff</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.605170</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>didnt</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.912023</td>\n",
              "      <td>7.824046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>went</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.506558</td>\n",
              "      <td>10.519674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>viewer</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.605170</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>deliv</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.605170</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>stori</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.605170</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>depress</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.605170</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>uninspir</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.605170</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209</th>\n",
              "      <td>deriv</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.605170</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>210 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        words   tf       idf     tf*idf\n",
              "0        want  1.0  4.605170   4.605170\n",
              "1        tiff  1.0  4.605170   4.605170\n",
              "2       didnt  2.0  3.912023   7.824046\n",
              "3        went  3.0  3.506558  10.519674\n",
              "4      viewer  1.0  4.605170   4.605170\n",
              "..        ...  ...       ...        ...\n",
              "205     deliv  1.0  4.605170   4.605170\n",
              "206     stori  1.0  4.605170   4.605170\n",
              "207   depress  1.0  4.605170   4.605170\n",
              "208  uninspir  1.0  4.605170   4.605170\n",
              "209     deriv  1.0  4.605170   4.605170\n",
              "\n",
              "[210 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHo2G5Mw6rcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cosine similarity for all the documents\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import numpy.linalg as LA\n",
        "train_set = tf2['clean_title'].values.tolist()\n",
        "test_set = \"As a viewer that actually went to TIFF and witnessed this film and didn't want to believe the hype, it is an absolute MASTERPIECE and Phoenix is a certified legend.\"\n",
        "test_set = [test_set]\n",
        "stopWords = stopwords.words('english')\n",
        "vectorizer = CountVectorizer(stop_words = stopWords)\n",
        "transformer = TfidfTransformer()\n",
        "trainVectorizerArray = vectorizer.fit_transform(train_set).toarray()\n",
        "testVectorizerArray = vectorizer.transform(test_set).toarray()\n",
        "cx = lambda a, b : np.inner(a, b)/(LA.norm(a)*LA.norm(b))\n",
        "result = []\n",
        "for vector in trainVectorizerArray:\n",
        "        for testV in testVectorizerArray:\n",
        "            cosine = cx(vector, testV)\n",
        "            result.append(cosine)           "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I69wiJ7ikBos",
        "colab_type": "code",
        "outputId": "34a083d2-fc82-44b9-f8d5-2eac0089734e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "source": [
        "#ranking the dicuments based on  cosine similarity\n",
        "new = tf2.filter(['Unnamed','clean_title'], axis=1)\n",
        "se = pd.Series(result)\n",
        "new['Cosine_similarity'] = se.values\n",
        "new.drop(new.loc[new['Cosine_similarity']==0].index, inplace=True)\n",
        "new[\"Rank\"] = new[\"Cosine_similarity\"].rank().astype(int)\n",
        "new.sort_values(\"Cosine_similarity\", inplace = True) \n",
        "new\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_title</th>\n",
              "      <th>Cosine_similarity</th>\n",
              "      <th>Rank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>worthi act perform phoenix worth watch joker s...</td>\n",
              "      <td>0.088388</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>probabl one best comic book movi perform phoen...</td>\n",
              "      <td>0.102062</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>far best dc movi date joaquin phoenix perfect</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>masterpiec movi year best actor joaquin phoenix</td>\n",
              "      <td>0.133631</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>went blind didnt enjoy realli disappoint</td>\n",
              "      <td>0.144338</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>overr badli direct film mislead titl</td>\n",
              "      <td>0.144338</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>joaquin phoenix deliv stori doesnt</td>\n",
              "      <td>0.158114</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>made account rate disappoint film</td>\n",
              "      <td>0.158114</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>went second time watch</td>\n",
              "      <td>0.176777</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>great act terribl film</td>\n",
              "      <td>0.176777</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>good film noth joker</td>\n",
              "      <td>0.176777</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>hat joaquin phoenix</td>\n",
              "      <td>0.204124</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>dont believ hype</td>\n",
              "      <td>0.204124</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>believ hype</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>ok film</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>oscar phoenix</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hype real</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>garbag hype</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>viewer actual went tiff wit film didnt want be...</td>\n",
              "      <td>0.730297</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          clean_title  Cosine_similarity  Rank\n",
              "36  worthi act perform phoenix worth watch joker s...           0.088388     1\n",
              "35  probabl one best comic book movi perform phoen...           0.102062     2\n",
              "92      far best dc movi date joaquin phoenix perfect           0.125000     3\n",
              "82    masterpiec movi year best actor joaquin phoenix           0.133631     4\n",
              "87           went blind didnt enjoy realli disappoint           0.144338     5\n",
              "65               overr badli direct film mislead titl           0.144338     5\n",
              "93                 joaquin phoenix deliv stori doesnt           0.158114     7\n",
              "47                  made account rate disappoint film           0.158114     7\n",
              "7                              went second time watch           0.176777    10\n",
              "79                             great act terribl film           0.176777    10\n",
              "78                               good film noth joker           0.176777    10\n",
              "83                                hat joaquin phoenix           0.204124    12\n",
              "56                                   dont believ hype           0.204124    12\n",
              "24                                        believ hype           0.250000    16\n",
              "22                                            ok film           0.250000    16\n",
              "15                                      oscar phoenix           0.250000    16\n",
              "4                                           hype real           0.250000    16\n",
              "57                                        garbag hype           0.250000    16\n",
              "0   viewer actual went tiff wit film didnt want be...           0.730297    19"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV",
        "colab_type": "text"
      },
      "source": [
        "# **Question 3: Create your own training and evaluation data for sentiment analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX",
        "colab_type": "text"
      },
      "source": [
        "(15 points). **You dodn't need to write program for this question!** Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfvMKJjIXS5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "https://github.com/akshithamaddi/Akshitha_INFO5731_Spring2020/blob/master/Assignment5_File_5731.csv"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}